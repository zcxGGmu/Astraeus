"""
æ¨¡å‹åç§°å’Œé…ç½®è§£æå·¥å…·

ç»Ÿä¸€å¤„ç†æ¨¡å‹åç§°è§£æã€API Key å’Œ API Base URL çš„é€»è¾‘
"""

import os
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple
from dotenv import load_dotenv
from utils.config import config
from utils.constants import MODEL_NAME_ALIASES

logger = logging.getLogger(__name__)

# ç¡®ä¿åŠ è½½ç¯å¢ƒå˜é‡ - .env æ–‡ä»¶åœ¨ backend ç›®å½•
current_dir = Path(__file__).resolve().parent  # utils ç›®å½•
backend_dir = current_dir.parent  # backend ç›®å½•
env_file = backend_dir / ".env"  # .env æ–‡ä»¶åœ¨ backend ç›®å½•

if env_file.exists():
    load_dotenv(dotenv_path=env_file, override=True)

else:
    logger.warning(f".env file not found at: {env_file}")
    # å°è¯•ä»å½“å‰ç›®å½•åŠ è½½
    load_dotenv(override=True)


class ModelConfig:
    """æ¨¡å‹é…ç½®æ•°æ®ç±»"""
    def __init__(
        self,
        model_name: str,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        provider: str = "Unknown",
        base_url: Optional[str] = None,  # ä¸“é—¨ç”¨äº Ollama çš„ base_url
    ):
        self.model_name = model_name
        self.api_key = api_key
        self.api_base = api_base
        self.provider = provider
        self.base_url = base_url  # Ollama ä½¿ç”¨


def resolve_model_config(model_name: Optional[str] = None) -> ModelConfig:
    """
    è§£ææ¨¡å‹é…ç½®ï¼Œç»Ÿä¸€å¤„ç†æ¨¡å‹åç§°ã€API Key å’Œ API Base
    
    Args:
        model_name: åŸå§‹æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        
    Returns:
        ModelConfig: åŒ…å«å®Œæ•´æ¨¡å‹é…ç½®çš„å¯¹è±¡
        
    Raises:
        ValueError: å½“å¿…éœ€çš„é…ç½®ç¼ºå¤±æ—¶
    """
    # 1. å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹åç§°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if model_name is None:
        model_name = config.MODEL_TO_USE
        logger.info(f"No model name provided, using default model: {model_name}")
    
    # 2. ç‰¹æ®Šå¤„ç†ï¼šOllama æ¨¡å‹
    if model_name == "ollama":
        return _resolve_ollama_config()
    
    # TODO: 3. å…¶ä»–ç‰¹æ®Šæ¨¡å‹å¤„ç†ï¼ˆvllmã€SGLang ç­‰ï¼‰
    # if model_name == "vllm":
    #     return _resolve_vllm_config()
    
    # 4. å¤„ç†æ¨¡å‹åç§°åˆ«åæ˜ å°„ï¼ˆè·³è¿‡å·²ç»ç‰¹æ®Šå¤„ç†è¿‡çš„æ¨¡å‹ï¼‰
    if not model_name.startswith("ollama_chat/"):
        resolved_model = MODEL_NAME_ALIASES.get(model_name, model_name)
        if resolved_model != model_name:
            logger.info(f"Model alias mapping: {model_name} -> {resolved_model}")
        model_name = resolved_model
    
    # 5. ç‰¹æ®Šå¤„ç† DeepSeek æ¨¡å‹æ ¼å¼
    if "DeepSeek" in model_name and "/" in model_name:
        logger.warning(f"Detected uppercase DeepSeek format: {model_name}, converting to standard format")
        model_name = "deepseek/deepseek-chat"
        logger.info(f"Converted to: {model_name}")
    
    # 6. æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæä¾›å•†å¹¶è·å– API Key å’Œ API Base
    api_key = None
    api_base = None
    provider = "Unknown"
    
    if "openai" in model_name.lower() or "gpt" in model_name.lower():
        api_key = config.OPENAI_API_KEY
        provider = "OpenAI"
        
    elif "anthropic" in model_name.lower() or "claude" in model_name.lower():
        api_key = config.ANTHROPIC_API_KEY
        provider = "Anthropic"
        
    elif "deepseek" in model_name.lower():
        # ä¼˜å…ˆä½¿ç”¨ DEEPSEEK_API_KEYï¼Œå›é€€åˆ° OPENAI_API_KEY
        api_key = getattr(config, 'DEEPSEEK_API_KEY', None) or config.OPENAI_API_KEY
        api_base = config.DEEPSEEK_API_BASE
        provider = "DeepSeek" if getattr(config, 'DEEPSEEK_API_KEY', None) else "DeepSeek (using OpenAI key)"
        
    elif "openrouter" in model_name.lower():
        api_key = config.OPENAI_API_KEY  # OpenRouter é€šå¸¸ä¹Ÿç”¨ç±»ä¼¼çš„ API Key
        api_base = config.OPENROUTER_API_BASE
        provider = "OpenRouter"
        
    elif "ollama" in model_name.lower():
        api_key = config.OLLAMA_API_KEY
        provider = "Ollama"
        # å¦‚æœæ¨¡å‹ååŒ…å« ollama_chat/ å‰ç¼€ï¼Œä»ç¯å¢ƒå˜é‡è¯»å– base_url
        if model_name.startswith("ollama_chat/"):
            ollama_base_url = os.getenv("OLLAMA_BASE_URL")
            if ollama_base_url:
                return ModelConfig(
                    model_name=model_name,
                    api_key=api_key,
                    api_base=None,
                    provider=provider,
                    base_url=ollama_base_url,
                )
            else:
                logger.warning("OLLAMA_BASE_URL not found in environment variables")
        
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI
        api_key = config.OPENAI_API_KEY
        provider = "OpenAI (default)"
        logger.warning(f"Unrecognized model {model_name}, using default OpenAI configuration")
    
    logger.info(f"Model resolved: {model_name}")
    logger.info(f"Provider: {provider}")
    logger.info(f"API Key: {'***' + api_key[-4:] if api_key else 'None'}")
    if api_base:
        logger.info(f"API Base: {api_base}")
    
    return ModelConfig(
        model_name=model_name,
        api_key=api_key,
        api_base=api_base,
        provider=provider,
    )


def _resolve_ollama_config() -> ModelConfig:
    """
    è§£æ Ollama æ¨¡å‹é…ç½®
    
    ä»ç¯å¢ƒå˜é‡è¯»å– OLLAMA_MODEL_NAME å’Œ OLLAMA_BASE_URLï¼Œ
    å¹¶æ‹¼æ¥æˆ ollama_chat/{model_name} æ ¼å¼
    
    Returns:
        ModelConfig: Ollama æ¨¡å‹é…ç½®
        
    Raises:
        ValueError: å½“å¿…éœ€çš„ç¯å¢ƒå˜é‡ç¼ºå¤±æ—¶
    """
    logger.info("ğŸ” Resolving Ollama configuration from environment variables")
    
    ollama_model_name = os.getenv("OLLAMA_MODEL_NAME")
    ollama_base_url = os.getenv("OLLAMA_BASE_URL")
    
    if not ollama_model_name:
        logger.error("OLLAMA_MODEL_NAME not found in environment variables")
        raise ValueError("OLLAMA_MODEL_NAME not configured")
    
    if not ollama_base_url:
        logger.error("OLLAMA_BASE_URL not found in environment variables")
        raise ValueError("OLLAMA_BASE_URL not configured")
    
    # æ‹¼æ¥æˆ ollama_chat æ ¼å¼
    model_name = f"ollama_chat/{ollama_model_name}"
    
    result = ModelConfig(
        model_name=model_name,
        api_key=config.OLLAMA_API_KEY,
        api_base=None,
        provider="Ollama",
        base_url=ollama_base_url,  # Ollama ç‰¹æœ‰çš„ base_url
    )
        
    return result


# TODO: æœªæ¥å¯ä»¥æ·»åŠ å…¶ä»–æ¨¡å‹æä¾›å•†çš„è§£æå‡½æ•°
# def _resolve_vllm_config() -> ModelConfig:
#     """è§£æ vLLM æ¨¡å‹é…ç½®"""
#     pass
#
# def _resolve_sglang_config() -> ModelConfig:
#     """è§£æ SGLang æ¨¡å‹é…ç½®"""
#     pass
